{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c088dde",
   "metadata": {},
   "source": [
    "## 1. Purpose of Linear Regression\n",
    "Linear regression models the relationship between a **target variable** and one or more **predictor variables**.\n",
    "\n",
    "**Why we use it:**\n",
    "- Predict a continuous value (price, demand, revenue).\n",
    "- Understand how each feature influences the target.\n",
    "- Establish a baseline model before using more complex methods.\n",
    "\n",
    "**Key idea:** Fit a straight line that best explains the data trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Simple example: y = 2x\n",
    "# X must be 2D for sklearn, so reshape\n",
    "X = np.array([[1],[2],[3],[4]])\n",
    "y = np.array([2,4,6,8])\n",
    "\n",
    "# Fit linear regression\n",
    "model = LinearRegression().fit(X,y)\n",
    "\n",
    "# coef_ is slope, intercept_ is the constant term\n",
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5758512e",
   "metadata": {},
   "source": [
    "## 2. Collinearity\n",
    "Collinearity means **two variables move together in a predictable way**.\n",
    "\n",
    "**Examples:**\n",
    "- Engine size and horsepower increase together.\n",
    "- Height and weight often increase together.\n",
    "\n",
    "Collinearity is about **two variables** being related. It becomes a problem when it grows into **multicollinearity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1897a",
   "metadata": {},
   "source": [
    "## 3. Multicollinearity\n",
    "Multicollinearity occurs when **three or more predictors are highly correlated**, or when one predictor can be predicted from a combination of others.\n",
    "\n",
    "**Why it's a problem:**\n",
    "- Coefficients become unstable.\n",
    "- Model becomes sensitive to small data changes.\n",
    "- Hard to interpret feature importance.\n",
    "\n",
    "**Example:**\n",
    "- `engine_size`, `horsepower`, and `torque` all strongly related.\n",
    "- The model struggles to decide which one is truly important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b174abe",
   "metadata": {},
   "source": [
    "## 4. Correlation Plots\n",
    "Correlation plots help visualize relationships between variables.\n",
    "\n",
    "**Use cases:**\n",
    "- Detect collinearity and multicollinearity.\n",
    "- Understand feature relationships.\n",
    "- Quick EDA before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2343d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load a simple dataset\n",
    "df = sns.load_dataset('iris').drop(columns=['species'])\n",
    "\n",
    "# Heatmap shows correlation between features\n",
    "# High values (close to 1 or -1) indicate strong relationships\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e91a2c",
   "metadata": {},
   "source": [
    "## 5. Remedies for Multicollinearity\n",
    "- Remove one of the correlated features.\n",
    "- Combine features (averages, ratios).\n",
    "- Use **PCA** to reduce dimensionality.\n",
    "- Use **regularization** (Ridge, LASSO, Elastic Net).\n",
    "- Increase dataset size if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adeb3f7",
   "metadata": {},
   "source": [
    "## 6. Principal Component Analysis (PCA)\n",
    "PCA reduces dimensionality by transforming features into **uncorrelated components**.\n",
    "\n",
    "**Why PCA helps:**\n",
    "- Removes multicollinearity.\n",
    "- Reduces noise.\n",
    "- Speeds up training.\n",
    "\n",
    "**Key idea:** Rotate the feature space to new axes (principal components) that capture maximum variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit PCA on iris numeric features\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(df)\n",
    "\n",
    "# Shows how much variance each component captures\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e90dd",
   "metadata": {},
   "source": [
    "## 7. Bias vs Variance\n",
    "**Bias:** Error from overly simple assumptions (underfitting).\n",
    "\n",
    "**Variance:** Error from overly complex models (overfitting).\n",
    "\n",
    "**Goal:** Find the balance — a model that generalizes well.\n",
    "\n",
    "**High bias model:** Linear regression with too few features.\n",
    "\n",
    "**High variance model:** Polynomial regression with high degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcffe25",
   "metadata": {},
   "source": [
    "## 8. Regularization\n",
    "Regularization prevents **overfitting** by adding a penalty to large coefficients.\n",
    "\n",
    "### Why overfitting happens\n",
    "- The model learns noise instead of patterns.\n",
    "- Coefficients become extremely large.\n",
    "- Model performs well on training data but poorly on new data.\n",
    "\n",
    "### What regularization does\n",
    "- Shrinks coefficients toward zero.\n",
    "- Reduces model complexity.\n",
    "- Improves generalization.\n",
    "- Helps with multicollinearity.\n",
    "\n",
    "### Real‑world examples\n",
    "**1. Car price prediction**\n",
    "- Features like horsepower, engine size, and torque are correlated.\n",
    "- Regularization stabilizes coefficients.\n",
    "\n",
    "**2. Marketing budget optimization**\n",
    "- Many channels overlap (Google Ads, Meta Ads, Display Ads).\n",
    "- Regularization prevents over‑crediting one channel.\n",
    "\n",
    "**3. Healthcare risk scoring**\n",
    "- BMI, weight, waist size are correlated.\n",
    "- Regularization avoids unstable predictions.\n",
    "\n",
    "Types of regularization:\n",
    "- **L1 (LASSO)** → feature selection.\n",
    "- **L2 (Ridge)** → coefficient shrinkage.\n",
    "- **Elastic Net** → combination of L1 + L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77f983",
   "metadata": {},
   "source": [
    "## 9. LASSO Regression (L1 Regularization)\n",
    "LASSO adds an **L1 penalty** (absolute value of coefficients).\n",
    "\n",
    "### Key properties\n",
    "- Can shrink some coefficients **exactly to zero**.\n",
    "- Performs **automatic feature selection**.\n",
    "- Useful when you have many features.\n",
    "\n",
    "**Marketing attribution**: Out of 50 campaign features, LASSO identifies the top 5–10 that truly drive conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d253930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# LASSO applies L1 penalty\n",
    "# This encourages some coefficients to become exactly zero\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Coefficients after L1 shrinkage\n",
    "# If any value becomes 0, LASSO removed that feature\n",
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c98a13",
   "metadata": {},
   "source": [
    "## 10. Ridge Regression (L2 Regularization)\n",
    "Ridge adds an **L2 penalty** (square of coefficients).\n",
    "\n",
    "### Key properties\n",
    "- Shrinks coefficients but **never sets them to zero**.\n",
    "- Best when all features are useful.\n",
    "- Excellent for **multicollinearity**.\n",
    "\n",
    "**Car price prediction**: Engine size, horsepower, and torque are correlated. Ridge stabilizes the model by distributing weights more evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Ridge applies L2 penalty\n",
    "# This reduces coefficient magnitude but keeps all features\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a124c3",
   "metadata": {},
   "source": [
    "## 11. Elastic Net (L1 + L2 Regularization)\n",
    "Elastic Net combines **L1 (LASSO)** and **L2 (Ridge)** penalties.\n",
    "\n",
    "### Key properties\n",
    "- Performs feature selection (L1).\n",
    "- Stabilizes coefficients (L2).\n",
    "- Works well when features are correlated.\n",
    "\n",
    "**Customer churn prediction**: Many behavioral features overlap. Elastic Net selects important ones while keeping the model stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Elastic Net blends L1 and L2 penalties\n",
    "# l1_ratio controls the mix: 0 = pure Ridge, 1 = pure LASSO\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "enet.fit(X, y)\n",
    "\n",
    "enet.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7eabc4",
   "metadata": {},
   "source": [
    "## 12. Polynomial Regression\n",
    "Polynomial regression models **non‑linear relationships** by adding polynomial terms.\n",
    "\n",
    "**Example:** Instead of fitting a straight line, fit a curve.\n",
    "\n",
    "**Risk:** High‑degree polynomials can overfit (high variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Polynomial regression of degree 2\n",
    "# Adds x^2 term automatically\n",
    "poly_model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('lr', LinearRegression())\n",
    "])\n",
    "\n",
    "poly_model.fit(X, y)\n",
    "\n",
    "# Coefficients for 1, x, x^2\n",
    "poly_model.named_steps['lr'].coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
