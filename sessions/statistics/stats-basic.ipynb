{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is Statistics?\n",
        "\n",
        "**Statistics** is the science of:\n",
        "- Collecting data\n",
        "- Organizing data\n",
        "- Summarizing data\n",
        "- Analyzing data\n",
        "- Drawing conclusions from data\n",
        "\n",
        "It is used in many fields like marketing, business, healthcare, telecom, and data science.\n",
        "\n",
        "**Real-life example:**\n",
        "- You track your monthly expenses and calculate the average to understand your spending.\n",
        "\n",
        "**Production example:**\n",
        "- A company collects application logs, response times, and user behavior,\n",
        "  then uses statistics to understand performance and user patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Types of Statistics\n",
        "\n",
        "There are two main branches:\n",
        "\n",
        "1. **Descriptive Statistics** – describe and summarize data.\n",
        "2. **Inferential Statistics** – use sample data to make conclusions about a larger population using probability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Basic Terms: Population, Sample, Variable, Parameter, Statistic\n",
        "\n",
        "- **Population**: The entire group you care about.\n",
        "- **Sample**: A subset of the population, used for analysis.\n",
        "- **Variable**: A characteristic that can vary (e.g., height, response time).\n",
        "- **Parameter**: A numerical summary of the population (e.g., true mean of all users).\n",
        "- **Statistic**: A numerical summary of the sample (e.g., mean of sampled users).\n",
        "\n",
        "**Real-life example:**\n",
        "- Population: All people in a city.\n",
        "- Sample: 500 people selected for a survey.\n",
        "\n",
        "**Production example:**\n",
        "- Population: All API calls in a month.\n",
        "- Sample: Logs from a single day.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Types of Data\n",
        "\n",
        "### 4.1 Categorical (Qualitative) Data\n",
        "Represents **categories or groups**.\n",
        "\n",
        "Examples:\n",
        "- Car brands: `\"Audi\"`, `\"BMW\"`, `\"Mercedes\"`\n",
        "- Yes/No answers\n",
        "- Browser type: `\"Chrome\"`, `\"Firefox\"`, `\"Safari\"`\n",
        "\n",
        "### 4.2 Numerical (Quantitative) Data\n",
        "Represents **numbers**.\n",
        "\n",
        "It can be:\n",
        "- **Discrete**: Countable values (e.g., number of children, number of defects).\n",
        "- **Continuous**: Measured values (e.g., height, time, distance, latency).\n",
        "\n",
        "**Production example:**\n",
        "- Discrete: Number of failed test cases in a test run.\n",
        "- Continuous: API response time in milliseconds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple example: classifying data as categorical, discrete, or continuous\n",
        "\n",
        "data_examples = {\n",
        "    \"car_brand\": \"BMW\",          # categorical\n",
        "    \"num_children\": 2,           # discrete numerical\n",
        "    \"response_time_ms\": 187.5    # continuous numerical\n",
        "}\n",
        "\n",
        "for name, value in data_examples.items():\n",
        "    print(f\"{name}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Levels of Measurement\n",
        "\n",
        "1. **Nominal** – categories without order (e.g., colors, browser type).\n",
        "2. **Ordinal** – ordered categories (e.g., ratings: bad, ok, good).\n",
        "3. **Interval** – numeric scale without a true zero (e.g., °C, °F).\n",
        "4. **Ratio** – numeric scale with a true zero (e.g., height, weight, response time).\n",
        "\n",
        "These levels determine which statistical methods and operations make sense.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part A: Descriptive Statistics\n",
        "\n",
        "- Measures of central tendency (mean, median, mode)\n",
        "- Measures of dispersion (range, variance, standard deviation)\n",
        "- Skewness (shape of the distribution)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Measures of Central Tendency\n",
        "\n",
        "### 6.1 Mean\n",
        "The **mean** is the arithmetic average.\n",
        "- Mean is heavily affected by **outliers**.\n",
        "\n",
        "### 6.2 Median\n",
        "The **median** is the middle value when data is sorted.\n",
        "- If `n` is odd → exact middle value.\n",
        "- If `n` is even → average of two middle values.\n",
        "- Median is more robust when there are extreme values.\n",
        "\n",
        "### 6.3 Mode\n",
        "The **mode** is the most frequent value in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Example dataset: systolic blood pressure of 7 men\n",
        "bp = np.array([150, 123, 134, 170, 146, 124, 113])\n",
        "\n",
        "mean_bp = np.mean(bp)\n",
        "median_bp = np.median(bp)\n",
        "mode_bp = stats.mode(bp, keepdims=True)\n",
        "\n",
        "print(\"Data:\", bp)\n",
        "print(\"Mean:\", mean_bp)\n",
        "print(\"Median:\", median_bp)\n",
        "print(\"Mode:\", mode_bp.mode[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Effect of Outliers on Mean vs Median\n",
        "\n",
        "Add a very large outlier and compare mean and median.\n",
        "\n",
        "- Mean will shift a lot.\n",
        "- Median will barely move.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "expenditure = np.random.normal(25000, 15000, 10000)\n",
        "\n",
        "print(\"Original mean:\", np.mean(expenditure))\n",
        "print(\"Original median:\", np.median(expenditure))\n",
        "\n",
        "# Add a huge outlier\n",
        "expenditure_with_outlier = np.append(expenditure, [10_000_000_000])\n",
        "\n",
        "print(\"\\nAfter adding outlier:\")\n",
        "print(\"New mean:\", np.mean(expenditure_with_outlier))\n",
        "print(\"New median:\", np.median(expenditure_with_outlier))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Measures of Dispersion: Range, Variance, Standard Deviation\n",
        "\n",
        "Measures of dispersion describe **how spread out** the data is around its center (usually the mean or median).  \n",
        "They help in understanding whether the data points are **close together** or **widely scattered**.\n",
        "\n",
        "Dispersion is important because **two datasets can have the same mean but very different spreads**, leading to very different interpretations.\n",
        "\n",
        "### 7.1 Range\n",
        "\n",
        "**Definition:**  \n",
        "The range is the simplest measure of dispersion.  \n",
        "It is the difference between the **largest** and **smallest** values in the dataset.\n",
        "\n",
        "**Formula:**  \n",
        "Range = maximum value − minimum value\n",
        "\n",
        "**What it tells you:**  \n",
        "- Gives a quick sense of the total spread  \n",
        "- Very sensitive to outliers  \n",
        "- Doesn’t show how values are distributed between the extremes  \n",
        "\n",
        "**Real-life examples:**  \n",
        "- Temperature difference in a day (high − low)  \n",
        "- Height difference in a classroom  \n",
        "- Monthly stock price high vs low  \n",
        "\n",
        "**Engineering examples:**  \n",
        "- Fastest vs slowest API response time in a day  \n",
        "- Minimum vs maximum CPU usage during a load test  \n",
        "- Smallest vs largest packet size in network traffic  \n",
        "\n",
        "### 7.2 Variance\n",
        "\n",
        "**Definition:**  \n",
        "Variance measures the **average of the squared differences** between each data point and the mean.\n",
        "\n",
        "It answers the question:  \n",
        "**“On average, how far are the data points from the mean — but squared?”**\n",
        "\n",
        "**Why squared?**  \n",
        "- Prevents negative values from canceling out  \n",
        "- Penalizes larger deviations more strongly  \n",
        "- Makes the math work nicely for probability and machine learning  \n",
        "\n",
        "**Interpretation:**  \n",
        "- High variance → data points are far from the mean  \n",
        "- Low variance → data points are close to the mean  \n",
        "\n",
        "**Real-life examples:**  \n",
        "- Variance in daily steps: high variance means inconsistent activity  \n",
        "- Variance in exam scores: high variance means some students did very well and some very poorly  \n",
        "\n",
        "**Engineering examples:**  \n",
        "- Variance in API latency: high variance means unpredictable performance  \n",
        "- Variance in memory usage: helps detect instability or memory leaks  \n",
        "- Variance in manufacturing measurements: used in quality control (Six Sigma)  \n",
        "\n",
        "### 7.3 Standard Deviation (SD)\n",
        "\n",
        "**Definition:**  \n",
        "Standard deviation is the **square root of variance**.  \n",
        "It brings the measure back to the **same units** as the original data, making it easier to interpret.\n",
        "\n",
        "**Why SD is more useful than variance:**  \n",
        "- Variance is in squared units (e.g., ms², kg²), which is hard to interpret  \n",
        "- SD is in the original units (ms, kg), so it makes intuitive sense  \n",
        "\n",
        "**Interpretation:**  \n",
        "- **Small SD** → data is tightly clustered around the mean  \n",
        "- **Large SD** → data is widely spread  \n",
        "- **SD = 0** → all values are identical  \n",
        "\n",
        "**Real-life examples:**  \n",
        "- Low SD in monthly expenses → stable spending habits  \n",
        "- High SD in commute time → unpredictable traffic  \n",
        "\n",
        "**Engineering examples:**  \n",
        "- Low SD in API response time → stable system  \n",
        "- High SD in test execution time → flaky tests  \n",
        "- Low SD in sensor readings → reliable hardware  \n",
        "- High SD in error counts → unstable system behavior  \n",
        "\n",
        "## Why Dispersion Matters\n",
        "\n",
        "Two datasets can have the **same mean** but behave completely differently:\n",
        "\n",
        "Example:  \n",
        "Dataset A: 50, 51, 49, 50, 52  \n",
        "Dataset B: 10, 90, 5, 95, 50  \n",
        "\n",
        "Both have a mean around 50, but Dataset B is far more spread out.  \n",
        "Dispersion helps in seeing this difference clearly.\n",
        "\n",
        "- **Range** → Quick snapshot of total spread  \n",
        "- **Variance** → Mathematical measure of average squared deviation  \n",
        "- **Standard Deviation** → Practical, easy-to-interpret measure of spread  \n",
        "\n",
        "Together, these metrics help in understanding **consistency**, **stability**, and **variability** in any dataset — whether it's exam scores, financial data, or system performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = np.array([3, 3, 3, 5, 6, 1])\n",
        "\n",
        "sample_mean = np.mean(results)\n",
        "sample_var = np.var(results, ddof=1)  # ddof=1 for sample variance\n",
        "sample_std = np.std(results, ddof=1)  # sample standard deviation\n",
        "data_range = np.max(results) - np.min(results)\n",
        "\n",
        "print(\"Data:\", results)\n",
        "print(\"Mean:\", sample_mean)\n",
        "print(\"Range:\", data_range)\n",
        "print(\"Sample variance:\", sample_var)\n",
        "print(\"Sample standard deviation:\", sample_std)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizing Spread with Standard Deviation\n",
        "\n",
        "We can visualize how data points cluster around the mean and how standard deviation reflects the spread.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Dataset with small standard deviation\n",
        "data_small_std = np.random.normal(100, 5, 1000)\n",
        "\n",
        "# Dataset with large standard deviation\n",
        "data_large_std = np.random.normal(100, 20, 1000)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.histplot(data_small_std, kde=True, ax=axes[0], color='green')\n",
        "axes[0].set_title('Small Standard Deviation')\n",
        "axes[0].axvline(np.mean(data_small_std), color='red', linestyle='--', label='Mean')\n",
        "axes[0].legend()\n",
        "\n",
        "sns.histplot(data_large_std, kde=True, ax=axes[1], color='orange')\n",
        "axes[1].set_title('Large Standard Deviation')\n",
        "axes[1].axvline(np.mean(data_large_std), color='red', linestyle='--', label='Mean')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Skewness\n",
        "\n",
        "Skewness measures how **asymmetric** a distribution is.\n",
        "\n",
        "- **Skewness ≈ 0** → roughly symmetric.\n",
        "- **Positive skew** → right tail is longer (few very large values).\n",
        "- **Negative skew** → left tail is longer (few very small values).\n",
        "\n",
        "**Production example:**\n",
        "- Latency distribution is often positively skewed: most requests are fast, but a few are very slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: symmetric vs positively skewed data\n",
        "\n",
        "from scipy.stats import skew\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "symmetric_data = np.random.normal(0, 1, 1000)\n",
        "positive_skew_data = np.random.exponential(scale=1.0, size=1000)\n",
        "\n",
        "print(\"Skewness (symmetric):\", skew(symmetric_data))\n",
        "print(\"Skewness (positive skew):\", skew(positive_skew_data))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.histplot(symmetric_data, kde=True, ax=axes[0], color='blue')\n",
        "axes[0].set_title('Symmetric Distribution')\n",
        "\n",
        "sns.histplot(positive_skew_data, kde=True, ax=axes[1], color='purple')\n",
        "axes[1].set_title('Positively Skewed Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Variables and Distributions\n",
        "\n",
        "**probability distributions**: which describe how random variables behave.\n",
        "\n",
        "- Random variables\n",
        "- Discrete vs continuous\n",
        "- Bernoulli distribution\n",
        "- Binomial distribution\n",
        "- Normal (Gaussian) distribution\n",
        "- Poisson distribution (brief)\n",
        "- PDF, PMF, and CDF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Random Variables\n",
        "\n",
        "A **random variable** is a variable whose value depends on the outcome of a random experiment.\n",
        "\n",
        "- **Discrete random variable**: takes countable values (0, 1, 2, 3, ...).\n",
        "- **Continuous random variable**: takes any value in an interval.\n",
        "\n",
        "**Examples:**\n",
        "- Discrete: number of defective items in a batch.\n",
        "- Continuous: time taken for a request to complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Bernoulli Distribution\n",
        "\n",
        "The **Bernoulli distribution** is the simplest possible probability distribution.  \n",
        "It describes a situation where there are **only two possible outcomes**:\n",
        "\n",
        "- **1 (success)** with probability `p`\n",
        "- **0 (failure)** with probability `1 - p`\n",
        "\n",
        "This makes Bernoulli perfect for modeling **yes/no**, **true/false**, **pass/fail**, or **on/off** events.\n",
        "\n",
        "In short:\n",
        "\n",
        "- You perform one trial.\n",
        "- The trial can only result in success or failure.\n",
        "- You assign a probability `p` to success.\n",
        "- Everything else is failure.\n",
        "\n",
        " **Why Bernoulli Matters**\n",
        "\n",
        "Bernoulli is the **building block** for many other distributions:\n",
        "\n",
        "- **Binomial distribution** = repeated Bernoulli trials  \n",
        "- **Geometric distribution** = number of Bernoulli trials until first success  \n",
        "- **Logistic regression** models Bernoulli outcomes  \n",
        "- **Binary classification** in machine learning uses Bernoulli assumptions  \n",
        "\n",
        "Whenever you see a binary outcome, you are looking at a Bernoulli process.\n",
        "\n",
        "**Coin Toss**  \n",
        "If you define “success = heads”, then:\n",
        "- Heads → 1  \n",
        "- Tails → 0  \n",
        "\n",
        "**Exam Pass/Fail**  \n",
        "- Pass → 1  \n",
        "- Fail → 0  \n",
        "\n",
        "**Light Bulb Works or Not**  \n",
        "- Working bulb → 1  \n",
        "- Defective bulb → 0  \n",
        "\n",
        "**Customer Buys or Doesn’t Buy**  \n",
        "- Purchase → 1  \n",
        "- No purchase → 0  \n",
        "\n",
        "**Traffic Light Detection**  \n",
        "- Light is green → 1  \n",
        "- Light is not green → 0  \n",
        "\n",
        "These are all **single-event, two-outcome** situations.\n",
        "\n",
        "**Engineering Examples:**\n",
        "\n",
        "**API Request Outcome**  \n",
        "- Success (HTTP 200) → 1  \n",
        "- Failure (HTTP 4xx/5xx) → 0  \n",
        "\n",
        "This is one of the most common Bernoulli processes in systems engineering.\n",
        "\n",
        "**Email Campaign**  \n",
        "- User opens email → 1  \n",
        "- User does not open → 0  \n",
        "\n",
        "CTR (click-through rate) is literally the **mean of Bernoulli outcomes**.\n",
        "\n",
        "**Feature Flag Check**  \n",
        "- Feature enabled → 1  \n",
        "- Feature disabled → 0  \n",
        "\n",
        "**Authentication**  \n",
        "- Login success → 1  \n",
        "- Login failure → 0  \n",
        "\n",
        "**Fraud Detection**  \n",
        "- Transaction is fraudulent → 1  \n",
        "- Transaction is legitimate → 0  \n",
        "\n",
        "**Sensor Trigger**  \n",
        "- Motion detected → 1  \n",
        "- No motion → 0  \n",
        "\n",
        "**Test Case Execution**  \n",
        "- Test passes → 1  \n",
        "- Test fails → 0  \n",
        "\n",
        "**What Bernoulli Really Represents**\n",
        "\n",
        "A Bernoulli trial is like asking a **single yes/no question**:\n",
        "\n",
        "- “Did the event happen?”  \n",
        "- “Did the user click?”  \n",
        "- “Did the request succeed?”  \n",
        "- “Did the sensor detect motion?”  \n",
        "\n",
        "If the answer is yes → 1  \n",
        "If the answer is no → 0  \n",
        "\n",
        "The probability of “yes” is `p`.  \n",
        "The probability of “no” is `1 - p`.\n",
        "\n",
        "**Mean and Variance**\n",
        "\n",
        "For a Bernoulli random variable:\n",
        "\n",
        "- **Mean = p**  \n",
        "  (the long-term proportion of successes)\n",
        "\n",
        "- **Variance = p(1 - p)**  \n",
        "  (highest when p = 0.5, lowest when p = 0 or 1)\n",
        "\n",
        "This is why:\n",
        "- A coin toss (p = 0.5) is the most “uncertain” Bernoulli trial  \n",
        "- A nearly guaranteed event (p ≈ 1) has very low variance  \n",
        "\n",
        "Engineering usage:\n",
        "\n",
        "- A/B testing  \n",
        "- Machine learning (binary classification)  \n",
        "- Reliability engineering  \n",
        "- QA pass/fail analysis  \n",
        "- Monitoring and alerting  \n",
        "- User behavior modeling  \n",
        "- Risk analysis  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import bernoulli\n",
        "\n",
        "p = 0.7  # probability of success\n",
        "bern_samples = bernoulli.rvs(p, size=20, random_state=42)\n",
        "\n",
        "print(\"Bernoulli samples (1 = success, 0 = failure):\")\n",
        "print(bern_samples)\n",
        "\n",
        "print(\"Mean (approx p):\", np.mean(bern_samples))\n",
        "print(\"Variance (approx p(1-p)):\", np.var(bern_samples, ddof=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Binomial Distribution\n",
        "\n",
        "The binomial distribution describes the number of **successes** you get when you repeat the same yes/no experiment a fixed number of times. Each attempt is a Bernoulli trial, meaning it can only result in success (1) or failure (0).\n",
        "\n",
        "A situation follows a binomial distribution when:\n",
        "- You repeat the experiment a fixed number of times (`n`).\n",
        "- Each trial has only two outcomes (success or failure).\n",
        "- The probability of success (`p`) stays the same for every trial.\n",
        "- Each trial is independent of the others.\n",
        "\n",
        "In simple terms, the binomial distribution answers the question:\n",
        "\n",
        "**“If I repeat this binary event n times, how many successes should I expect?”**\n",
        "\n",
        "This makes it useful for anything involving repeated yes/no outcomes.\n",
        "\n",
        "**Real-life examples:**\n",
        "\n",
        "- **Coin tossing:** Toss a coin 10 times and count how many heads appear.  \n",
        "- **Exam guessing:** A student guesses on 20 multiple-choice questions; how many will they get right?  \n",
        "- **Sports:** A basketball player makes 70% of free throws; how many shots will they make out of 10 attempts?  \n",
        "- **Manufacturing:** A machine has a 2% defect rate; how many defective items will appear in a batch of 50?\n",
        "\n",
        "These are all repeated Bernoulli trials with a fixed number of attempts.\n",
        "\n",
        "**Engineering and production examples:**\n",
        "\n",
        "- **User conversions:** If the signup conversion rate is `p`, how many signups will occur out of 1000 visitors?  \n",
        "- **Email campaigns:** If 30% of users open an email, how many opens will you get from 500 emails?  \n",
        "- **API reliability:** If an API succeeds 98% of the time, how many successful responses will occur in 200 calls?  \n",
        "- **QA testing:** If a flaky test passes 90% of the time, how many passes will you see in 50 runs?  \n",
        "- **Feature rollouts:** If 5% of users click a new feature, how many clicks will come from 10,000 users?  \n",
        "- **Security monitoring:** If 1% of login attempts are suspicious, how many suspicious attempts will appear in 5000 logins?\n",
        "\n",
        "All of these involve repeating the same binary event many times and counting how often success occurs.\n",
        "\n",
        "**Why the binomial distribution matters:**\n",
        "\n",
        "It is one of the most widely used distributions in statistics because it models real-world binary outcomes repeated many times. It forms the basis for:\n",
        "\n",
        "- A/B testing  \n",
        "- Conversion rate modeling  \n",
        "- Reliability and failure analysis  \n",
        "- Quality control  \n",
        "- Risk estimation  \n",
        "- Machine learning classification probabilities  \n",
        "\n",
        "Whenever you count how many times something “works” out of a fixed number of attempts, you are using the binomial distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import binom\n",
        "\n",
        "n = 10  # number of trials\n",
        "p = 0.5 # probability of success\n",
        "\n",
        "# Probability of exactly 5 successes\n",
        "prob_5 = binom.pmf(5, n, p)\n",
        "print(\"P(X = 5) when n=10, p=0.5:\", prob_5)\n",
        "\n",
        "# Simulate many binomial outcomes\n",
        "simulated = binom.rvs(n=n, p=p, size=1000, random_state=42)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(simulated, bins=range(n+2), discrete=True, stat='probability', color='skyblue')\n",
        "plt.title('Binomial Distribution Simulation (n=10, p=0.5)')\n",
        "plt.xlabel('Number of successes')\n",
        "plt.ylabel('Probability')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Normal Distribution (Gaussian)\n",
        "\n",
        "The **normal distribution** is a continuous distribution with a bell-shaped curve.\n",
        "\n",
        "Key properties:\n",
        "- Symmetric around the mean.\n",
        "- Mean = Median = Mode.\n",
        "- Shape controlled by mean `μ` and standard deviation `σ`.\n",
        "\n",
        "### Empirical Rule (68-95-99.7 rule)\n",
        "- About **68%** of data lies within 1σ of mean.\n",
        "- About **95%** within 2σ.\n",
        "- About **99.7%** within 3σ.\n",
        "\n",
        "**Real-life example:**\n",
        "- Height of people.\n",
        "\n",
        "**Production example:**\n",
        "- If response times follow roughly a normal distribution, most requests fall near the mean with predictable spread.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "mu = 100\n",
        "sigma = 15\n",
        "\n",
        "x = np.linspace(mu - 4*sigma, mu + 4*sigma, 500)\n",
        "y = norm.pdf(x, mu, sigma)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(x, y, label='Normal PDF')\n",
        "plt.axvline(mu, color='red', linestyle='--', label='Mean')\n",
        "plt.axvline(mu - sigma, color='gray', linestyle=':', label='μ ± σ')\n",
        "plt.axvline(mu + sigma, color='gray', linestyle=':')\n",
        "plt.title('Normal Distribution (μ = 100, σ = 15)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Standard Normal Distribution and Z-Score\n",
        "\n",
        "The **standard normal distribution** is a special normal distribution with:\n",
        "- Mean = 0\n",
        "- Standard deviation = 1\n",
        "\n",
        "We convert any normally distributed variable X into a standard normal variable Z by using the z‑score formula.\n",
        "The z‑score is calculated as:\n",
        "\n",
        "**z = (x − mean) / standard deviation**\n",
        "\n",
        "\n",
        "Interpretation:\n",
        "- z = 0 → value equals the mean.\n",
        "- z = 1 → one standard deviation above the mean.\n",
        "- z = -2 → two standard deviations below the mean.\n",
        "\n",
        "**Production example:**\n",
        "- Compute z-score for latency or error counts to detect anomalies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: exam scores with mean 72, sd 2.0\n",
        "mu = 72\n",
        "sigma = 2.0\n",
        "stephanie_score = 74\n",
        "\n",
        "z_stephanie = (stephanie_score - mu) / sigma\n",
        "print(\"Stephanie's z-score:\", z_stephanie)\n",
        "\n",
        "# Daniel: mean 76, sd 4.5, score 64\n",
        "mu2 = 76\n",
        "sigma2 = 4.5\n",
        "daniel_score = 64\n",
        "z_daniel = (daniel_score - mu2) / sigma2\n",
        "print(\"Daniel's z-score:\", z_daniel)\n",
        "\n",
        "# Probability of scoring below a certain z (using CDF)\n",
        "prob_steph_below = norm.cdf(z_stephanie)\n",
        "print(\"Probability of scoring <= Stephanie's score:\", prob_steph_below)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Poisson Distribution\n",
        "\n",
        "The Poisson distribution is used to predict **how many times an event will occur** within a fixed amount of time, space, or volume. It applies when events:\n",
        "\n",
        "- happen independently of each other, and  \n",
        "- occur at a steady average rate over time.\n",
        "\n",
        "In simple terms, Poisson helps estimate **how many events you can expect** when those events happen randomly but follow a consistent long‑term average.\n",
        "\n",
        "**Everyday and Real‑World Uses**\n",
        "- Calls arriving at a call center per minute  \n",
        "- Cars passing through a toll booth in a given time  \n",
        "- Customers entering a store during an hour  \n",
        "- Earthquakes above a certain magnitude in a year  \n",
        "- Accidents occurring at a specific intersection each month  \n",
        "\n",
        "These are all cases where events are random but follow a predictable long‑term rate.\n",
        "\n",
        "**Engineering and System-Level Uses**\n",
        "- Errors appearing in logs within a time window  \n",
        "- Failed API calls in a batch of requests  \n",
        "- Jobs arriving in a message queue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import poisson\n",
        "\n",
        "lam = 3  # average number of events per interval\n",
        "x_values = np.arange(0, 11)\n",
        "poisson_probs = poisson.pmf(x_values, lam)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.stem(x_values, poisson_probs, use_line_collection=True)\n",
        "plt.title('Poisson Distribution (λ = 3)')\n",
        "plt.xlabel('Number of events (x)')\n",
        "plt.ylabel('P(X = x)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. PDF, PMF, and CDF\n",
        "\n",
        "When working with probability distributions, we often need to describe how likely different outcomes are. Three core concepts help us do this: **PMF**, **PDF**, and **CDF**. They tell us how probability is distributed across values of a random variable.\n",
        "\n",
        "**PMF (Probability Mass Function)**  \n",
        "A PMF is used for **discrete** random variables — variables that take specific, countable values.  \n",
        "It tells you the probability of the variable being exactly equal to a particular value.\n",
        "\n",
        "Examples of discrete distributions:  \n",
        "- Binomial (number of successes in n trials)  \n",
        "- Poisson (number of events in a time interval)\n",
        "\n",
        "**IT engineering examples:**  \n",
        "- Number of failed API calls in a batch of 100 requests  \n",
        "- Number of errors logged in the last minute  \n",
        "- Number of retries needed before a request succeeds  \n",
        "- Number of users who click a button out of 50 shown the feature  \n",
        "- Number of packets dropped in a network router per second  \n",
        "\n",
        "In all these cases, the outcome is a **count**, so PMF applies.\n",
        "\n",
        "**PDF (Probability Density Function)**  \n",
        "A PDF is used for **continuous** random variables — variables that can take any value within a range.  \n",
        "A PDF does *not* give the probability of a single exact value (because that probability is effectively zero).  \n",
        "Instead, it describes how probability is **distributed across intervals**.\n",
        "\n",
        "Examples of continuous distributions:  \n",
        "- Normal distribution  \n",
        "- Exponential distribution  \n",
        "- Log-normal distribution  \n",
        "\n",
        "**IT engineering examples:**  \n",
        "- API response time (e.g., 123.45 ms)  \n",
        "- CPU utilization percentage (e.g., 67.2%)  \n",
        "- Memory usage in MB  \n",
        "- Network latency in milliseconds  \n",
        "- Time between two incoming requests  \n",
        "\n",
        "These values can take infinitely many possible numbers, so PDFs describe their behavior.\n",
        "\n",
        "**CDF (Cumulative Distribution Function)**  \n",
        "A CDF gives the probability that a random variable is **less than or equal to** a certain value.  \n",
        "It accumulates probability from the left side of the distribution up to that point.\n",
        "\n",
        "CDF is extremely useful for understanding **percentiles**.\n",
        "\n",
        "**IT engineering examples:**  \n",
        "- Latency percentiles (P50, P90, P95, P99)  \n",
        "  - CDF tells you what percentage of requests are faster than a given latency.  \n",
        "- Error rate thresholds  \n",
        "  - Probability that errors per minute stay below a certain limit.  \n",
        "- Queue wait times  \n",
        "  - Probability that a job waits less than X milliseconds.  \n",
        "- Disk I/O performance  \n",
        "  - Probability that read/write completes within a certain time.  \n",
        "- Cloud autoscaling  \n",
        "  - Probability that CPU stays below 80% for the next 5 minutes.  \n",
        "\n",
        "CDF is the backbone of **SRE metrics**, **SLAs**, and **performance dashboards**.\n",
        "\n",
        "**Putting it all together in IT engineering:**\n",
        "\n",
        "- Use **PMF** when counting events (errors, retries, failures, user clicks).  \n",
        "- Use **PDF** when measuring continuous metrics (latency, CPU, memory, throughput).  \n",
        "- Use **CDF** when analyzing percentiles, thresholds, or SLA compliance.\n",
        "\n",
        "These three concepts help engineers understand system behavior, detect anomalies, and make data-driven decisions about performance, reliability, and scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: CDF for Normal distribution\n",
        "\n",
        "mu = 100\n",
        "sigma = 15\n",
        "\n",
        "values = [85, 100, 115]\n",
        "for v in values:\n",
        "    z = (v - mu) / sigma\n",
        "    prob = norm.cdf(v, mu, sigma)\n",
        "    print(f\"P(X <= {v}) = {prob:.4f} (z = {z:.2f})\")\n",
        "\n",
        "# Visualizing the CDF\n",
        "x = np.linspace(mu - 4*sigma, mu + 4*sigma, 500)\n",
        "cdf_values = norm.cdf(x, mu, sigma)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(x, cdf_values, label='Normal CDF')\n",
        "plt.title('CDF of Normal Distribution (μ = 100, σ = 15)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('P(X ≤ x)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Central Limit Theorem (High-Level)\n",
        "\n",
        "The Central Limit Theorem (CLT) explains why the **Normal distribution appears everywhere**, even when the original data is messy, skewed, or irregular.\n",
        "\n",
        "- You repeatedly take samples from any population (as long as the population has a finite variance).  \n",
        "- For each sample, you calculate the **mean**.  \n",
        "- If you collect enough sample means, their distribution will start to look **normal (bell‑shaped)**.  \n",
        "- This happens **even if the original data is not normal at all**.\n",
        "\n",
        "**When you average things, the averages tend to become normally distributed.**\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "- **Daily average API latency:**  \n",
        "  Individual request latencies may be highly skewed (some very slow outliers), but the *daily average latency* across thousands of requests tends to follow a normal distribution.\n",
        "\n",
        "- **Average CPU usage per hour:**  \n",
        "  CPU usage at any moment jumps around unpredictably, but hourly averages form a bell-shaped pattern.\n",
        "\n",
        "- **Average memory consumption per container:**  \n",
        "  Instantaneous memory usage is noisy, but the average over many samples becomes normally distributed.\n",
        "\n",
        "- **Average number of errors per minute across a full day:**  \n",
        "  Error bursts cause spikes, but the distribution of *minute-level averages* across many days becomes normal.\n",
        "\n",
        "- **Average throughput of a microservice:**  \n",
        "  Per-request throughput varies wildly, but the average throughput over 5-minute windows tends to be normal.\n",
        "\n",
        "- **Average queue wait time:**  \n",
        "  Individual wait times may be chaotic, but the average wait time per hour becomes predictable and normal-like.\n",
        "\n",
        "- **Average disk I/O time:**  \n",
        "  Raw I/O times are often skewed, but averages over many operations follow a normal distribution.\n",
        "\n",
        "**Why engineers care:**\n",
        "\n",
        "Because of the CLT, you can:\n",
        "\n",
        "- Use normal-based confidence intervals for averages  \n",
        "- Predict system performance more reliably  \n",
        "- Detect anomalies using z-scores  \n",
        "- Build dashboards that rely on percentiles and averages  \n",
        "- Model aggregated metrics with normal assumptions  \n",
        "- Simplify complex, messy data into something predictable  \n",
        "\n",
        "The CLT is the reason why **averages are stable**, **percentiles make sense**, and **SRE metrics behave nicely** even when raw data does not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple CLT simulation\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Skewed population: exponential distribution\n",
        "population = np.random.exponential(scale=1.0, size=100000)\n",
        "\n",
        "sample_means = []\n",
        "sample_size = 50\n",
        "num_samples = 1000\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    sample = np.random.choice(population, size=sample_size, replace=False)\n",
        "    sample_means.append(np.mean(sample))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(sample_means, kde=True, color='teal')\n",
        "plt.title('Distribution of Sample Means (CLT demo)')\n",
        "plt.xlabel('Sample mean')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
